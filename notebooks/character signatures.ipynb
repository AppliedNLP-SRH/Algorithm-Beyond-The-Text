{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Character Analysis: Collocates & PMI\n",
    "\n",
    "## Description\n",
    "This notebook analyzes character signatures in novels by identifying words that frequently co-occur with specific characters (Collocates) and calculating Pointwise Mutual Information (PMI) to find statistically significant associations.\n",
    "\n",
    "## Requirements\n",
    "1.  **Tokenization & Cleaning:** Sentence splitting, stopword removal, lemmatization.\n",
    "2.  **Collocate Extraction:** Window-based context analysis (Â±5 words).\n",
    "3.  **PMI Calculation:** Statistical measure of association strength.\n",
    "4.  **Visualization:** Bar charts (Frequency vs. PMI) and Word Clouds.\n",
    "5.  **Export:** Results saved as CSV and PNG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Define paths based on your repository structure\n",
    "DATA_DIR = 'data'\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "# Configuration for analysis\n",
    "WINDOW_SIZE = 5  # +/- 5 tokens\n",
    "PMI_FREQ_THRESHOLD = 5  # Word must appear at least this many times to be counted for PMI\n",
    "TOP_N = 20  # Number of top words to visualize\n",
    "\n",
    "# Define the books and the target characters for each\n",
    "# Note: Filenames must match exactly what is in your data folder\n",
    "BOOKS_CONFIG = {\n",
    "    \"Anna Karenina\": {\n",
    "        \"filename\": \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\",\n",
    "        \"characters\": [\"Anna\", \"Vronsky\", \"Levin\", \"Kitty\", \"Karenin\"]\n",
    "    },\n",
    "    \"War and Peace\": {\n",
    "        \"filename\": \"The Project Gutenberg eBook of War and Peace, by Leo Tolstoy.txt\",\n",
    "        \"characters\": [\"Pierre\", \"Natasha\", \"Andrei\", \"Rostov\", \"Mary\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Functions\n",
    "Here we define functions to load text, clean it (lemmatization, stopword removal), and tokenize it into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath):\n",
    "    \"\"\"Reads the text file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    1. Splits into sentences.\n",
    "    2. Tokenizes words.\n",
    "    3. Lowercases.\n",
    "    4. Removes punctuation and stopwords.\n",
    "    5. Lemmatizes.\n",
    "    Returns: \n",
    "      - full_token_list: List of all processed tokens (for frequency counts)\n",
    "      - tokenized_sentences: List of lists (for context window analysis)\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Split into sentences first\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    processed_sentences = []\n",
    "    full_token_list = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Remove punctuation and split\n",
    "        # We keep alphanumeric tokens only\n",
    "        words = word_tokenize(sentence)\n",
    "        clean_words = []\n",
    "        \n",
    "        for w in words:\n",
    "            if w.isalnum():\n",
    "                w_lower = w.lower()\n",
    "                if w_lower not in stop_words:\n",
    "                    lemma = lemmatizer.lemmatize(w_lower)\n",
    "                    clean_words.append(lemma)\n",
    "                    full_token_list.append(lemma)\n",
    "        \n",
    "        if clean_words:\n",
    "            processed_sentences.append(clean_words)\n",
    "            \n",
    "    return full_token_list, processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Analytics (Collocates & PMI)\n",
    "These functions handle the logic for finding context words and calculating the PMI statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocates(target_char_names, tokenized_sentences, window=5):\n",
    "    \"\"\"\n",
    "    Finds words appearing within +/- window of the character name.\n",
    "    target_char_names: List of variations (e.g., [\"anna\", \"karenina\"])\n",
    "    \"\"\"\n",
    "    # Normalize target names to lower case\n",
    "    targets = [t.lower() for t in target_char_names]\n",
    "    \n",
    "    collocates = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word in targets:\n",
    "                # Define window range\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sentence), i + window + 1)\n",
    "                \n",
    "                # Grab context (excluding the character name itself)\n",
    "                context = sentence[start:i] + sentence[i+1:end]\n",
    "                collocates.extend(context)\n",
    "                \n",
    "    return collocates\n",
    "\n",
    "def calculate_pmi(char_collocates, total_corpus_tokens, top_n=20):\n",
    "    \"\"\"\n",
    "    Calculates PMI for character collocates.\n",
    "    PMI(w, c) = log( P(w|c) / P(w) )\n",
    "    \n",
    "    Interpretation: How much more likely is 'w' to appear near the character\n",
    "    than it is to appear randomly in the book?\n",
    "    \"\"\"\n",
    "    collocate_counts = Counter(char_collocates)\n",
    "    corpus_counts = Counter(total_corpus_tokens)\n",
    "    \n",
    "    total_collocates = len(char_collocates)\n",
    "    total_corpus = len(total_corpus_tokens)\n",
    "    \n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for word, count_in_context in collocate_counts.items():\n",
    "        # Apply frequency threshold to avoid noise\n",
    "        if count_in_context < PMI_FREQ_THRESHOLD:\n",
    "            continue\n",
    "        \n",
    "        # P(w|c) = count of w near c / total words near c\n",
    "        p_w_given_c = count_in_context / total_collocates\n",
    "        \n",
    "        # P(w) = count of w in whole book / total words in book\n",
    "        p_w = corpus_counts[word] / total_corpus\n",
    "        \n",
    "        if p_w > 0:\n",
    "            pmi = math.log(p_w_given_c / p_w)\n",
    "            pmi_scores[word] = pmi\n",
    "            \n",
    "    # Sort by highest PMI\n",
    "    sorted_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return sorted_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization & Export\n",
    "This section generates the graphs (Bar Charts, Word Clouds) and saves the data to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualizations(char_name, collocate_counts, pmi_scores, book_title):\n",
    "    \"\"\"Generates and saves bar charts and wordclouds.\"\"\"\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    top_freq = collocate_counts.most_common(TOP_N)\n",
    "    top_pmi = pmi_scores  # Already sorted and top N\n",
    "    \n",
    "    # Create a figure with 2 subplots (Frequency Bar, PMI Bar)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(f\"Character Signature: {char_name} ({book_title})\", fontsize=16)\n",
    "    \n",
    "    # Bar Chart: Frequency\n",
    "    words_f, counts_f = zip(*top_freq) if top_freq else ([], [])\n",
    "    axes[0].barh(words_f[::-1], counts_f[::-1], color='skyblue')\n",
    "    axes[0].set_title(f\"Top {TOP_N} Frequent Collocates\")\n",
    "    axes[0].set_xlabel(\"Frequency\")\n",
    "    \n",
    "    # Bar Chart: PMI\n",
    "    words_p, scores_p = zip(*top_pmi) if top_pmi else ([], [])\n",
    "    axes[1].barh(words_p[::-1], scores_p[::-1], color='salmon')\n",
    "    axes[1].set_title(f\"Top {TOP_N} PMI Collocates\")\n",
    "    axes[1].set_xlabel(\"PMI Score\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    \n",
    "    # Save Bar Charts\n",
    "    safe_name = char_name.lower().replace(\" \", \"_\")\n",
    "    safe_book = book_title.split()[0].lower()\n",
    "    filename = f\"{RESULTS_DIR}/{safe_book}_{safe_name}_bars.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Word Cloud (Frequency based)\n",
    "    if collocate_counts:\n",
    "        wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(collocate_counts)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Collocate Word Cloud: {char_name}\")\n",
    "        \n",
    "        wc_filename = f\"{RESULTS_DIR}/{safe_book}_{safe_name}_wordcloud.png\"\n",
    "        plt.savefig(wc_filename)\n",
    "        plt.close()\n",
    "\n",
    "def save_csv(char_name, collocate_counts, pmi_scores, book_title):\n",
    "    \"\"\"Exports raw data to CSV.\"\"\"\n",
    "    # Convert counters/lists to DataFrame\n",
    "    df_freq = pd.DataFrame(collocate_counts.most_common(), columns=['word', 'frequency'])\n",
    "    df_pmi = pd.DataFrame(pmi_scores, columns=['word', 'pmi_score'])\n",
    "    \n",
    "    # Merge for a clean view (Outer join to keep all)\n",
    "    df_merged = pd.merge(df_freq, df_pmi, on='word', how='outer')\n",
    "    \n",
    "    safe_name = char_name.lower().replace(\" \", \"_\")\n",
    "    safe_book = book_title.split()[0].lower()\n",
    "    csv_filename = f\"{RESULTS_DIR}/{safe_book}_{safe_name}_stats.csv\"\n",
    "    \n",
    "    df_merged.to_csv(csv_filename, index=False)\n",
    "    # print(f\"Saved CSV: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Execution\n",
    "Run this cell to process the books, generate all statistics, and save the results to the `results/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis():\n",
    "    print(\"Starting Analysis...\")\n",
    "    \n",
    "    for book_title, config in BOOKS_CONFIG.items():\n",
    "        print(f\"\\n--- Processing Book: {book_title} ---\")\n",
    "        filepath = os.path.join(DATA_DIR, config['filename'])\n",
    "        \n",
    "        # 1. Load\n",
    "        raw_text = load_text(filepath)\n",
    "        if not raw_text:\n",
    "            continue\n",
    "            \n",
    "        # 2. Global Preprocessing (Tokenize whole book once)\n",
    "        print(\"Tokenizing and cleaning text (this may take a moment)...\")\n",
    "        full_tokens, processed_sentences = clean_and_tokenize(raw_text)\n",
    "        print(f\"Total tokens: {len(full_tokens)}\")\n",
    "        \n",
    "        # 3. Analyze each character\n",
    "        for char_name in config['characters']:\n",
    "            print(f\"Analyzing character: {char_name}...\")\n",
    "            \n",
    "            # Get collocates (Words near the character)\n",
    "            # We look for the exact name (e.g. \"Anna\")\n",
    "            collocates = get_collocates([char_name], processed_sentences, window=WINDOW_SIZE)\n",
    "            \n",
    "            if not collocates:\n",
    "                print(f\"Warning: No occurrences found for {char_name}\")\n",
    "                continue\n",
    "                \n",
    "            collocate_counts = Counter(collocates)\n",
    "            \n",
    "            # Calculate PMI\n",
    "            pmi_results = calculate_pmi(collocates, full_tokens, top_n=TOP_N)\n",
    "            \n",
    "            # 4. Save Outputs\n",
    "            generate_visualizations(char_name, collocate_counts, pmi_results, book_title)\n",
    "            save_csv(char_name, collocate_counts, pmi_results, book_title)\n",
    "            \n",
    "    print(f\"\\nAnalysis Complete! Check the '{RESULTS_DIR}' folder.\")\n",
    "\n",
    "# Run the script\n",
    "run_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}