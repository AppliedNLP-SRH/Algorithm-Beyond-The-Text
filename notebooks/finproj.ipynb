# =========================================
#  FULL NLP ANALYSIS NOTEBOOK
#  Loads text from /mnt/data/, performs:
#  - Tokenization
#  - Lemmatization
#  - Stopwords removal
#  - Collocates (window-based)
#  - PMI computation
#  - Word frequencies
#  - Chapter detection + per-chapter stats
#  - Wordcloud + matplotlib charts
#  - CSV export
# =========================================

import os
import re
import math
import nltk
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from wordcloud import WordCloud

# -----------------------------------------
# SETTINGS
# -----------------------------------------

DATA_FILENAME = "text.txt"     # <-- Change if needed
CHARACTERS = {
    "Anna": ["Anna", "Karenina"],
    "Vronsky": ["Vronsky"],
    "Kitty": ["Kitty"],
}
WINDOW = 5
PMI_MIN_FREQ = 3

# -----------------------------------------
# LOAD TEXT
# -----------------------------------------

path = f"/mnt/data/{DATA_FILENAME}"
with open(path, "r", encoding="utf-8") as f:
    text = f.read()

print("Loaded text length:", len(text))

# -----------------------------------------
# NLP PREP
# -----------------------------------------

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

tokens_raw = nltk.word_tokenize(text)
sentences = nltk.sent_tokenize(text)

tokens = []
for w in tokens_raw:
    w = w.lower()
    if w.isalpha() and w not in stop_words:
        tokens.append(lemmatizer.lemmatize(w))

print("Total tokens:", len(tokens))

# Frequency table
word_freq = Counter(tokens)

# -----------------------------------------
# HELPER FOR COLLOCATES
# -----------------------------------------
def get_window_collocates(variants):
    collocates = Counter()
    idxs = [i for i, w in enumerate(tokens_raw) if w in variants]

    for idx in idxs:
        left = max(0, idx - WINDOW)
        right = min(len(tokens_raw), idx + WINDOW + 1)

        for w in tokens_raw[left:right]:
            lw = w.lower()
            if lw.isalpha() and lw not in stop_words and lw not in [v.lower() for v in variants]:
                lw = lemmatizer.lemmatize(lw)
                collocates[lw] += 1
    return collocates

# -----------------------------------------
# PMI
# -----------------------------------------
def compute_pmi(collocates, total_tokens):
    pmi = {}
    for word, freq in collocates.items():
        if freq < PMI_MIN_FREQ:
            continue
        p_xy = freq / total_tokens
        p_x = word_freq[word] / total_tokens
        p_y = freq / total_tokens
        pmi[word] = math.log2(p_xy / (p_x * p_y))
    return pmi

# -----------------------------------------
# CHARACTER ANALYSIS
# -----------------------------------------
results = {}
total_tokens = len(tokens)

for char, variants in CHARACTERS.items():
    col = get_window_collocates(variants)
    pmi = compute_pmi(col, total_tokens)

    df = pd.DataFrame({
        "word": list(col.keys()),
        "freq": list(col.values()),
        "PMI": [pmi.get(w, None) for w in col.keys()]
    }).sort_values("freq", ascending=False)

    df.to_csv(f"/mnt/data/{char}_collocates.csv", index=False)
    results[char] = df

print("Saved collocate CSVs for:", list(results.keys()))

# -----------------------------------------
# CHAPTER DETECTION
# -----------------------------------------
chapters = re.split(r"(?:CHAPTER|Chapter|chapter) [0-9IVXLC]+", text)
chapter_stats = []

for i, ch in enumerate(chapters[1:], start=1):
    toks = nltk.word_tokenize(ch)
    toks = [t.lower() for t in toks if t.isalpha() and t not in stop_words]
    toks = [lemmatizer.lemmatize(t) for t in toks]

    freq = Counter(toks)
    df = pd.DataFrame(freq.items(), columns=["word", "freq"]).sort_values("freq", ascending=False)
    df.to_csv(f"/mnt/data/chapter_{i}_freq.csv", index=False)

    chapter_stats.append((i, len(toks)))

chapter_df = pd.DataFrame(chapter_stats, columns=["chapter", "tokens"])
chapter_df.to_csv("/mnt/data/chapter_lengths.csv", index=False)

print("Chapter stats saved.")

# -----------------------------------------
# TOP WORD FREQUENCY PLOT
# -----------------------------------------
top_words = word_freq.most_common(20)
words = [w for w, _ in top_words]
counts = [c for _, c in top_words]

plt.figure(figsize=(10,5))
plt.bar(words, counts)
plt.xticks(rotation=45, ha="right")
plt.title("Top 20 Words")
plt.tight_layout()
plt.show()

# -----------------------------------------
# WORDCLOUD
# -----------------------------------------
wc = WordCloud(width=1000, height=600).generate(" ".join(tokens))

plt.figure(figsize=(12,6))
plt.imshow(wc)
plt.axis("off")
plt.show()

print("Done.")
