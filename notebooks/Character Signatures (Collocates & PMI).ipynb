{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment: Character Signatures (Collocates & PMI)\n",
    "\n",
    "## Goal\n",
    "Analyze the text to find **\"Character Signatures\"** for selected characters:\n",
    "\n",
    "1. **Frequency:** Words that appear most often near the character.\n",
    "2. **PMI (Pointwise Mutual Information):** Words that are statistically strongly associated with the character (\"unique\" to them relative to the whole book).\n",
    "\n",
    "## Output\n",
    "* **Visuals:** Side-by-side bar charts (Frequency vs. PMI) for each main character.\n",
    "* **Data:** CSV files with the full top lists for each character and metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup (Run Once)\n",
    "\n",
    "This cell installs any missing libraries (e.g. `wordcloud`) inside the current environment.\n",
    "\n",
    "You usually only need to run this cell **once per environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL MISSING LIBRARIES (run once, then you can comment this out)\n",
    "%pip install wordcloud pandas matplotlib nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration\n",
    "\n",
    "Set up paths, global parameters (window size, thresholds), and download NLTK resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- NLTK DATA (run once; safe to re-run) ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')  # some installations require this\n",
    "\n",
    "# --- PATHS ---\n",
    "# Assuming this notebook lives in `notebooks/` and the text is in `../data/`\n",
    "DATA_DIR = '../data'\n",
    "RESULTS_DIR = '../results'\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# --- ANALYSIS SETTINGS ---\n",
    "BOOK_FILENAME = 'anna_karenina.txt'   # file inside DATA_DIR\n",
    "BOOK_NAME = 'Anna Karenina'\n",
    "\n",
    "# Characters to analyze (as they appear after lowercasing / lemmatization)\n",
    "CHARACTERS = ['anna', 'levin']\n",
    "\n",
    "WINDOW_SIZE = 5          # +/- 5 words around the character\n",
    "PMI_FREQ_THRESHOLD = 5   # Only compute PMI for words that appear >= 5 times as collocates\n",
    "TOP_N = 10               # Top N words for graphs / CSV\n",
    "\n",
    "# Matplotlib default style tweaks (optional)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Loading & Preprocessing\n",
    "\n",
    "Steps:\n",
    "1. Read the raw text file.\n",
    "2. Split into sentences.\n",
    "3. Tokenize words.\n",
    "4. Lowercase, remove stopwords, keep only alphanumeric tokens.\n",
    "5. Lemmatize tokens.\n",
    "\n",
    "The function returns:\n",
    "* `full_tokens`: a flat list of all tokens in the book.\n",
    "* `processed_sentences`: a list of sentences, each as a list of cleaned tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(filename):\n",
    "    \"\"\"Load a text file and return:\n",
    "    - full_tokens: list of all cleaned tokens in the corpus\n",
    "    - processed_sentences: list of sentence-level token lists\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not find {filepath}\")\n",
    "        return [], []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    full_tokens = []\n",
    "\n",
    "    print(f\"Processing {len(sentences)} raw sentences from {filename}...\")\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        clean_words = []\n",
    "        for w in words:\n",
    "            # Keep only alphanumeric tokens (no pure punctuation)\n",
    "            if w.isalnum():\n",
    "                w_lower = w.lower()\n",
    "                if w_lower not in stop_words:\n",
    "                    lemma = lemmatizer.lemmatize(w_lower)\n",
    "                    clean_words.append(lemma)\n",
    "                    full_tokens.append(lemma)\n",
    "\n",
    "        if clean_words:\n",
    "            processed_sentences.append(clean_words)\n",
    "\n",
    "    print(f\"After cleaning: {len(processed_sentences)} sentences, {len(full_tokens)} tokens.\\n\")\n",
    "    return full_tokens, processed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collocates & PMI Computation\n",
    "\n",
    "### 3.1 Collocates\n",
    "For a given character token, we take a symmetric window of size `WINDOW_SIZE` around each occurrence and collect all surrounding words (excluding the character token itself).\n",
    "\n",
    "### 3.2 PMI\n",
    "PMI is computed as:\n",
    "\\begin{equation}\n",
    "PMI(w, c) = \\log \\frac{P(w \\mid c)}{P(w)}\n",
    "\\end{equation}\n",
    "where:\n",
    "* $P(w \\mid c)$ is the probability of seeing word *w* in the character's context windows.\n",
    "* $P(w)$ is the probability of seeing *w* anywhere in the corpus.\n",
    "\n",
    "We only compute PMI for collocates that occur at least `PMI_FREQ_THRESHOLD` times to avoid noisy, unstable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocates(target, sentences, window=5):\n",
    "    \"\"\"Collect all collocates for a target token within +/- `window` words.\n",
    "    `target` is expected to be lowercased and lemmatized.\n",
    "    \"\"\"\n",
    "    target = target.lower()\n",
    "    collocates = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word == target:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sent), i + window + 1)\n",
    "                # Add context (excluding the target word itself)\n",
    "                collocates.extend(sent[start:i] + sent[i+1:end])\n",
    "\n",
    "    return collocates\n",
    "\n",
    "\n",
    "def calculate_pmi(collocates, full_tokens, top_n=10):\n",
    "    \"\"\"Given collocates for a character and the full corpus tokens,\n",
    "    return (top_freq, top_pmi):\n",
    "      - top_freq: list of (word, count) sorted by frequency\n",
    "      - top_pmi: list of (word, pmi_score) sorted by PMI\n",
    "    \"\"\"\n",
    "    if not collocates or not full_tokens:\n",
    "        return [], []\n",
    "\n",
    "    collocate_counts = Counter(collocates)\n",
    "    corpus_counts = Counter(full_tokens)\n",
    "    total_collocates = len(collocates)\n",
    "    total_corpus = len(full_tokens)\n",
    "\n",
    "    pmi_scores = {}\n",
    "    for word, count in collocate_counts.items():\n",
    "        if count < PMI_FREQ_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        p_w_given_c = count / total_collocates\n",
    "        p_w = corpus_counts[word] / total_corpus\n",
    "\n",
    "        if p_w > 0:\n",
    "            # Natural log is fine; any base is acceptable as long as it's consistent\n",
    "            pmi_scores[word] = math.log(p_w_given_c / p_w)\n",
    "\n",
    "    # Top by raw frequency\n",
    "    top_freq = collocate_counts.most_common(top_n)\n",
    "\n",
    "    # Top by PMI score\n",
    "    top_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    return top_freq, top_pmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization & CSV Export\n",
    "\n",
    "### 4.1 Side-by-side bar charts\n",
    "For each character, we create the exact layout requested:\n",
    "\n",
    "* Left: **Top N collocates by frequency**.\n",
    "* Right: **Top N collocates by PMI**.\n",
    "\n",
    "### 4.2 CSV files\n",
    "For each character we also save two CSVs:\n",
    "\n",
    "* `<CHAR>_topN_frequency.csv`\n",
    "* `<CHAR>_topN_pmi.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assignment_graph(char_name, top_freq, top_pmi, book_name):\n",
    "    \"\"\"Create and save the side-by-side bar chart for frequency vs PMI.\"\"\"\n",
    "    if not top_freq or not top_pmi:\n",
    "        print(f\"[Warning] Not enough data to plot for {char_name}.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # 1. Frequency plot\n",
    "    words_f, counts_f = zip(*top_freq)\n",
    "    axes[0].barh(words_f[::-1], counts_f[::-1])\n",
    "    axes[0].set_title(f\"Top {TOP_N} Collocates (Frequency)\")\n",
    "    axes[0].set_xlabel(\"Count\")\n",
    "\n",
    "    # 2. PMI plot\n",
    "    words_p, scores_p = zip(*top_pmi)\n",
    "    axes[1].barh(words_p[::-1], scores_p[::-1])\n",
    "    axes[1].set_title(f\"Top {TOP_N} Collocates (PMI)\")\n",
    "    axes[1].set_xlabel(\"PMI value\")\n",
    "\n",
    "    plt.suptitle(f\"Character Signature: {char_name} ({book_name})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(RESULTS_DIR, f\"{char_name}_signature.png\")\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Graph saved to: {filename}\\n\")\n",
    "\n",
    "\n",
    "def save_results_to_csv(char_name, top_freq, top_pmi):\n",
    "    \"\"\"Save frequency and PMI results for a character as CSV files.\"\"\"\n",
    "    if top_freq:\n",
    "        df_freq = pd.DataFrame(top_freq, columns=[\"word\", \"count\"])\n",
    "        freq_path = os.path.join(RESULTS_DIR, f\"{char_name}_top{TOP_N}_frequency.csv\")\n",
    "        df_freq.to_csv(freq_path, index=False)\n",
    "        print(f\"Frequency CSV saved to: {freq_path}\")\n",
    "\n",
    "    if top_pmi:\n",
    "        df_pmi = pd.DataFrame(top_pmi, columns=[\"word\", \"pmi\"])\n",
    "        pmi_path = os.path.join(RESULTS_DIR, f\"{char_name}_top{TOP_N}_pmi.csv\")\n",
    "        df_pmi.to_csv(pmi_path, index=False)\n",
    "        print(f\"PMI CSV saved to: {pmi_path}\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Analysis\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Load and preprocess the book.\n",
    "2. For each character in `CHARACTERS`:\n",
    "   * Extract collocates within the specified window.\n",
    "   * Compute top collocates by frequency and PMI.\n",
    "   * Save the results as CSV.\n",
    "   * Plot the side-by-side bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "full_tokens, processed_sentences = load_and_clean(BOOK_FILENAME)\n",
    "\n",
    "if not full_tokens or not processed_sentences:\n",
    "    print(\"Aborting: could not load or process the text file.\")\n",
    "else:\n",
    "    for char in CHARACTERS:\n",
    "        print(f\"Analyzing {char.title()}...\\n\")\n",
    "\n",
    "        collocates = get_collocates(char, processed_sentences, window=WINDOW_SIZE)\n",
    "        print(f\"  Found {len(collocates)} collocate tokens for '{char}'.\")\n",
    "\n",
    "        top_freq, top_pmi = calculate_pmi(collocates, full_tokens, top_n=TOP_N)\n",
    "\n",
    "        # Show quick preview in the notebook\n",
    "        if top_freq:\n",
    "            print(\"  Top frequency collocates:\")\n",
    "            display(pd.DataFrame(top_freq, columns=[\"word\", \"count\"]))\n",
    "        if top_pmi:\n",
    "            print(\"  Top PMI collocates:\")\n",
    "            display(pd.DataFrame(top_pmi, columns=[\"word\", \"pmi\"]))\n",
    "\n",
    "        # CSV + plot\n",
    "        save_results_to_csv(char, top_freq, top_pmi)\n",
    "        create_assignment_graph(char.title(), top_freq, top_pmi, BOOK_NAME)\n",
    "\n",
    "    print(\"Done! Check the '../results' folder for PNGs and CSV files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
