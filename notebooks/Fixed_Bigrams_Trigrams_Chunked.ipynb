{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eef7e6",
   "metadata": {},
   "source": [
    "### FIX: Chunked n-gram extraction and plotting for very large books (War and Peace)\n",
    "\n",
    "Drop this cell **after** the cells that define `nlp`, `read_text`, `strip_gutenberg_headers`, `war_file`, `anna_file`, and `OUTDIR`.\n",
    "\n",
    "This cell replaces the in-memory single-call `nlp(text)` with a chunked `nlp.pipe` approach over paragraphs so spaCy won't choke on very large texts. It then re-runs the n-gram plots and saves PNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunked n-gram extraction + plotting cell\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def get_paragraph_chunks(text: str):\n",
    "    \"\"\"Split into paragraph-like chunks (based on blank lines), falling back to fixed-size character chunks\n",
    "    if paragraphs are excessively long. Returns a list of text chunks.\"\"\"\n",
    "    text = text.replace('\\r\\n', '\\n')\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    # If paragraphs are too large (e.g., one massive paragraph), fallback to slicing by character window\n",
    "    max_paragraph_length = 20000  # characters\n",
    "    chunks = []\n",
    "    for p in paragraphs:\n",
    "        if len(p) <= max_paragraph_length:\n",
    "            chunks.append(p)\n",
    "        else:\n",
    "            # split long paragraph into ~20k char chunks on whitespace boundaries\n",
    "            start = 0\n",
    "            L = len(p)\n",
    "            while start < L:\n",
    "                end = min(start + max_paragraph_length, L)\n",
    "                # try to extend end to next space to avoid breaking a word\n",
    "                if end < L:\n",
    "                    nxt = p.rfind(' ', start, end)\n",
    "                    if nxt > start:\n",
    "                        end = nxt\n",
    "                chunks.append(p[start:end].strip())\n",
    "                start = end\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_top_ngrams_from_text_chunked(text: str, n: int=2, top_k: int=20):\n",
    "    \"\"\"\n",
    "    Efficiently compute top n-grams from a very large text by processing chunks with nlp.pipe.\n",
    "    Keeps only alphabetic tokens (token.is_alpha) and lowercases them.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    chunks = get_paragraph_chunks(text)\n",
    "    # Use nlp.pipe for efficiency and to avoid building one enormous Doc\n",
    "    # Filter out empty chunks\n",
    "    chunks = [c for c in chunks if c]\n",
    "    if not chunks:\n",
    "        return []\n",
    "    # Process in batches\n",
    "    for doc in nlp.pipe(chunks, batch_size=50):\n",
    "        tokens = [t.text.lower() for t in doc if t.is_alpha]\n",
    "        if len(tokens) < n:\n",
    "            continue\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            gram = \" \".join(tokens[i:i+n])\n",
    "            counts[gram] += 1\n",
    "    return counts.most_common(top_k)\n",
    "\n",
    "\n",
    "def plot_bigrams_trigrams_for_book_chunked(book_title: str, text: str, top_k: int=15, savepath=None):\n",
    "    bigrams = get_top_ngrams_from_text_chunked(text, n=2, top_k=top_k)\n",
    "    trigrams = get_top_ngrams_from_text_chunked(text, n=3, top_k=top_k)\n",
    "\n",
    "    bigram_labels, bigram_vals = zip(*bigrams) if bigrams else ([], [])\n",
    "    trigram_labels, trigram_vals = zip(*trigrams) if trigrams else ([], [])\n",
    "\n",
    "    bigram_labels = list(bigram_labels)[::-1]\n",
    "    bigram_vals = list(bigram_vals)[::-1]\n",
    "    trigram_labels = list(trigram_labels)[::-1]\n",
    "    trigram_vals = list(trigram_vals)[::-1]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10), constrained_layout=True)\n",
    "\n",
    "    # Trigrams on top\n",
    "    ax = axes[0]\n",
    "    y_pos = range(len(trigram_labels))\n",
    "    ax.barh(y_pos, trigram_vals)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(trigram_labels, fontsize=9)\n",
    "    ax.set_xlabel(\"Frequency\")\n",
    "    ax.set_title(f\"{book_title} — Top {top_k} Trigrams\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Bigrams on bottom\n",
    "    ax = axes[1]\n",
    "    y_pos = range(len(bigram_labels))\n",
    "    ax.barh(y_pos, bigram_vals)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(bigram_labels, fontsize=9)\n",
    "    ax.set_xlabel(\"Frequency\")\n",
    "    ax.set_title(f\"{book_title} — Top {top_k} Bigrams\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    if savepath:\n",
    "        Path(savepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(savepath, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return savepath, fig\n",
    "\n",
    "# --- Run for War and Peace and Anna Karenina if variables exist ---\n",
    "# This cell assumes your notebook already has `read_text`, `strip_gutenberg_headers`, `war_file`, `anna_file`, and `OUTDIR` defined.\n",
    "\n",
    "if 'war_file' in globals() and war_file is not None and war_file.exists():\n",
    "    print('Processing War and Peace (chunked) ...')\n",
    "    war_text = strip_gutenberg_headers(read_text(war_file))\n",
    "    war_png = OUTDIR / 'War_and_Peace_bi_tri_ngrams_chunked.png'\n",
    "    plot_bigrams_trigrams_for_book_chunked('War and Peace', war_text, top_k=15, savepath=str(war_png))\n",
    "    print('Saved:', war_png)\n",
    "else:\n",
    "    print('war_file not found in globals or file does not exist; skipping War and Peace chunked plot.')\n",
    "\n",
    "if 'anna_file' in globals() and anna_file is not None and anna_file.exists():\n",
    "    print('Processing Anna Karenina (chunked) ...')\n",
    "    anna_text = strip_gutenberg_headers(read_text(anna_file))\n",
    "    anna_png = OUTDIR / 'Anna_Karenina_bi_tri_ngrams_chunked.png'\n",
    "    plot_bigrams_trigrams_for_book_chunked('Anna Karenina', anna_text, top_k=15, savepath=str(anna_png))\n",
    "    print('Saved:', anna_png)\n",
    "else:\n",
    "    print('anna_file not found in globals or file does not exist; skipping Anna Karenina chunked plot.')\n",
    "\n",
    "print('\\nIf War and Peace still fails to produce a plot, check the printed messages and verify the file path and that spaCy model is available.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
