{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment: Character Signatures (Collocates & PMI)\n",
    "\n",
    "## Goal\n",
    "Analyze the text to find \"Character Signatures\":\n",
    "1.  **Frequency:** Words that appear most often near the character.\n",
    "2.  **PMI (Pointwise Mutual Information):** Words that are statistically strongly associated with the character (unique to them).\n",
    "\n",
    "## Output\n",
    "* **Visuals:** Side-by-side Bar Charts (Frequency vs. PMI) for main characters.\n",
    "* **Data:** CSV files with full lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL MISSING LIBRARIES\n",
    "# Run this cell once to fix the 'ModuleNotFoundError: No module named wordcloud' error\n",
    "%pip install wordcloud pandas matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Path Setup (Going up one level from 'notebooks' to find 'data')\n",
    "DATA_DIR = '../data'\n",
    "RESULTS_DIR = '../results'\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "# Analysis Settings\n",
    "WINDOW_SIZE = 5         # +/- 5 words\n",
    "PMI_FREQ_THRESHOLD = 5  # Word must appear 5+ times to be considered for PMI\n",
    "TOP_N = 10              # Top 10 words for the graph (as per assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(filename):\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Could not find {filepath}\")\n",
    "        return [], []\n",
    "\n",
    "    # Initialize NLP tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    full_tokens = []\n",
    "    \n",
    "    print(f\"Processing {len(sentences)} sentences...\")\n",
    "    \n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        clean_words = []\n",
    "        for w in words:\n",
    "            if w.isalnum():\n",
    "                w_lower = w.lower()\n",
    "                if w_lower not in stop_words:\n",
    "                    lemma = lemmatizer.lemmatize(w_lower)\n",
    "                    clean_words.append(lemma)\n",
    "                    full_tokens.append(lemma)\n",
    "        if clean_words:\n",
    "            processed_sentences.append(clean_words)\n",
    "            \n",
    "    return full_tokens, processed_sentences\n",
    "\n",
    "def get_collocates(target, sentences, window=5):\n",
    "    target = target.lower()\n",
    "    collocates = []\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word == target:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sent), i + window + 1)\n",
    "                # Add context (excluding the target word itself)\n",
    "                collocates.extend(sent[start:i] + sent[i+1:end])\n",
    "    return collocates\n",
    "\n",
    "def calculate_pmi(collocates, full_tokens, top_n=10):\n",
    "    collocate_counts = Counter(collocates)\n",
    "    corpus_counts = Counter(full_tokens)\n",
    "    total_collocates = len(collocates)\n",
    "    total_corpus = len(full_tokens)\n",
    "    \n",
    "    pmi_scores = {}\n",
    "    for word, count in collocate_counts.items():\n",
    "        if count < PMI_FREQ_THRESHOLD: continue\n",
    "        \n",
    "        p_w_given_c = count / total_collocates\n",
    "        p_w = corpus_counts[word] / total_corpus\n",
    "        \n",
    "        if p_w > 0:\n",
    "            pmi_scores[word] = math.log(p_w_given_c / p_w)\n",
    "            \n",
    "    # Get Top Frequency and Top PMI\n",
    "    top_freq = collocate_counts.most_common(top_n)\n",
    "    top_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return top_freq, top_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Function\n",
    "Creates the exact side-by-side graph requested: Top Frequency on the left, Top PMI on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assignment_graph(char_name, top_freq, top_pmi, book_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 1. Frequency Plot\n",
    "    words_f, counts_f = zip(*top_freq)\n",
    "    axes[0].barh(words_f[::-1], counts_f[::-1], color='#4c72b0')\n",
    "    axes[0].set_title(f\"Top {TOP_N} Collocates (Frequency)\")\n",
    "    axes[0].set_xlabel(\"Count\")\n",
    "\n",
    "    # 2. PMI Plot\n",
    "    words_p, scores_p = zip(*top_pmi)\n",
    "    axes[1].barh(words_p[::-1], scores_p[::-1], color='#55a868')\n",
    "    axes[1].set_title(f\"Top {TOP_N} Collocates (PMI Score)\")\n",
    "    axes[1].set_xlabel(\"PMI Value\")\n",
    "\n",
    "    plt.suptitle(f\"Character Signature: {char_name} ({book_name})\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    filename = f\"{RESULTS_DIR}/{char_name}_signature.png\"\n",
    "    plt.savefig(filename)\n",
    "    print(f\"Graph saved to: {filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution\n",
    "We will analyze **Anna** and **Levin** from *Anna Karenina* to produce clean, representative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what to analyze\n",
    "BOOK_FILE = \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\"\n",
    "CHARACTERS = [\"Anna\", \"Levin\"] # The two main contrasts\n",
    "\n",
    "# 1. Load Data\n",
    "full_tokens, sentences = load_and_clean(BOOK_FILE)\n",
    "\n",
    "if full_tokens:\n",
    "    for char in CHARACTERS:\n",
    "        print(f\"\\nAnalyzing {char}...\")\n",
    "        \n",
    "        # 2. Get Context & Stats\n",
    "        collocates = get_collocates(char, sentences, WINDOW_SIZE)\n",
    "        if not collocates:\n",
    "            print(f\"Character {char} not found!\")\n",
    "            continue\n",
    "            \n",
    "        top_freq, top_pmi = calculate_pmi(collocates, full_tokens, TOP_N)\n",
    "        \n",
    "        # 3. Generate Graph\n",
    "        create_assignment_graph(char, top_freq, top_pmi, \"Anna Karenina\")\n",
    "        \n",
    "        # 4. Save CSV (Requirement)\n",
    "        df_f = pd.DataFrame(top_freq, columns=['Word', 'Frequency'])\n",
    "        df_p = pd.DataFrame(top_pmi, columns=['Word', 'PMI_Score'])\n",
    "        # Save separately or merged, here we save the top lists\n",
    "        df_f.to_csv(f\"{RESULTS_DIR}/{char}_freq.csv\", index=False)\n",
    "        df_p.to_csv(f\"{RESULTS_DIR}/{char}_pmi.csv\", index=False)\n",
    "\n",
    "print(\"\\nDone! Checked results folder for PNGs and CSVs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}