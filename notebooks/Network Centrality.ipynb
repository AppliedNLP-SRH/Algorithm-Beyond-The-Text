{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure 4: Network Centrality (\"The Hub\")\n",
    "\n",
    "## Goal\n",
    "Create a social network graph to visualize character interactions.\n",
    "\n",
    "## The Metrics\n",
    "* **Degree Centrality:** Measures the number of direct connections a character has. (The \"Popularity\" score).\n",
    "* **Betweenness Centrality:** Measures how often a character acts as a bridge between other groups. (The \"Broker\" score).\n",
    "\n",
    "## The Visualization\n",
    "* **Nodes (Dots):** Characters. Size = Centrality.\n",
    "* **Edges (Lines):** Co-occurrence in the same sentence.\n",
    "* **Target Outcome:** Anna should be central and large; Levin should be peripheral/isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL REQUIRED LIBRARIES\n",
    "# We need 'networkx' for graph theory calculations\n",
    "%pip install networkx pandas matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- SETUP ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# PATHS\n",
    "DATA_DIR = '../data'\n",
    "RESULTS_DIR = '../results'\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "# CONFIGURATION\n",
    "# We strictly map the characters mentioned in the assignment prompt.\n",
    "# To make the graph \"Hub\" effect visible, we include key connecting characters (like Stiva/Dolly).\n",
    "BOOKS_CONFIG = {\n",
    "    \"Anna Karenina\": {\n",
    "        \"filename\": \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\",\n",
    "        # Added Stiva/Dolly/Betsy to ensure the network has enough nodes to show structure\n",
    "        \"characters\": [\"Anna\", \"Vronsky\", \"Levin\", \"Kitty\", \"Karenin\", \"Stiva\", \"Dolly\", \"Betsy\"]\n",
    "    },\n",
    "    \"War and Peace\": {\n",
    "        \"filename\": \"The Project Gutenberg eBook of War and Peace, by Leo Tolstoy.txt\",\n",
    "        \"characters\": [\"Pierre\", \"Natasha\", \"Andrei\", \"Rostov\", \"Mary\", \"Helene\", \"Anatole\", \"Kutuzov\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Building Functions\n",
    "These functions scan the text sentence-by-sentence. If two characters appear in the same sentence, a link (edge) is created between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename):\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filepath} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "def build_interaction_network(text, char_list):\n",
    "    \"\"\"\n",
    "    Scans sentences. If two characters co-occur, add an edge.\n",
    "    Returns a NetworkX Graph object.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add all characters as nodes first (even if they have no connections)\n",
    "    for char in char_list:\n",
    "        G.add_node(char)\n",
    "        \n",
    "    # Normalize names for search\n",
    "    char_map = {c.lower(): c for c in char_list}\n",
    "    \n",
    "    print(f\"Scanning {len(sentences)} sentences for interactions...\")\n",
    "    \n",
    "    for sent in sentences:\n",
    "        # Tokenize and clean sentence slightly for better matching\n",
    "        tokens = set(word_tokenize(sent.lower()))\n",
    "        \n",
    "        # Find which characters are in this sentence\n",
    "        present_chars = [char_map[c] for c in char_map if c in tokens]\n",
    "        \n",
    "        # If 2 or more characters are present, draw edges between them\n",
    "        if len(present_chars) > 1:\n",
    "            # Create all pairs (combinations of 2)\n",
    "            for pair in itertools.combinations(present_chars, 2):\n",
    "                u, v = pair\n",
    "                # If edge exists, increase weight (strength of relationship)\n",
    "                if G.has_edge(u, v):\n",
    "                    G[u][v]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(u, v, weight=1)\n",
    "                    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization & Analysis\n",
    "This calculates the centrality scores and draws the network. The **node size** is dynamically adjusted based on the character's importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_visualize(G, book_title):\n",
    "    # 1. Calculate Centrality Metrics\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # 2. Export Data to CSV\n",
    "    df = pd.DataFrame({\n",
    "        'Character': degree_centrality.keys(),\n",
    "        'Degree_Centrality': degree_centrality.values(),\n",
    "        'Betweenness_Centrality': betweenness.values()\n",
    "    }).sort_values(by='Degree_Centrality', ascending=False)\n",
    "    \n",
    "    safe_title = book_title.split()[0].lower()\n",
    "    csv_path = f\"{RESULTS_DIR}/{safe_title}_network_metrics.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved metrics to {csv_path}\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # 3. Draw the Graph\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Layout: Spring layout uses force-directed algorithms \n",
    "    # (Central nodes are pulled to center, isolated ones pushed out)\n",
    "    pos = nx.spring_layout(G, k=0.8, seed=42) \n",
    "    \n",
    "    # Determine Node Size based on Degree Centrality\n",
    "    # Multiply by a factor (e.g., 5000) to make dots visible\n",
    "    node_sizes = [v * 5000 for v in degree_centrality.values()]\n",
    "    \n",
    "    # Determine Edge Width based on weight (frequency of interaction)\n",
    "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    # Normalize edge widths so they aren't too thick\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    edge_widths = [(w / max_weight) * 5 for w in weights]\n",
    "\n",
    "    # Draw Nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"skyblue\", alpha=0.9)\n",
    "    \n",
    "    # Draw Edges\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5, edge_color=\"gray\")\n",
    "    \n",
    "    # Draw Labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\")\n",
    "    \n",
    "    plt.title(f\"Network Centrality: {book_title}\\n(Node Size = Centrality)\", fontsize=15)\n",
    "    plt.axis('off') # Turn off axis numbers\n",
    "    \n",
    "    # Save Graph\n",
    "    img_path = f\"{RESULTS_DIR}/{safe_title}_network_graph.png\"\n",
    "    plt.savefig(img_path)\n",
    "    print(f\"Graph saved to {img_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network_analysis():\n",
    "    print(\"Starting Network Analysis...\")\n",
    "    \n",
    "    for book_title, config in BOOKS_CONFIG.items():\n",
    "        print(f\"\\n--- Processing {book_title} ---\")\n",
    "        \n",
    "        text = load_text(config['filename'])\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # Build Graph\n",
    "        G = build_interaction_network(text, config['characters'])\n",
    "        \n",
    "        if G.number_of_edges() == 0:\n",
    "            print(\"No interactions found! Check character names or file content.\")\n",
    "            continue\n",
    "            \n",
    "        # Visualize\n",
    "        analyze_and_visualize(G, book_title)\n",
    "\n",
    "run_network_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}