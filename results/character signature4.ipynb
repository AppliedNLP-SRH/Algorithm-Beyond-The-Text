{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Character Analysis: Presentation Graphs\n",
    "\n",
    "## Description\n",
    "This notebook analyzes character signatures and generates **three distinct graphs** for a presentation:\n",
    "1.  **Scatter Plot:** Frequency vs. PMI (to show common vs. unique words).\n",
    "2.  **Grouped Bar Chart:** Comparing specific themes between two characters.\n",
    "3.  **Word Cloud:** A visual summary of the main character's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# --- SETUP ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# PATHS (Adjusted for your folder structure)\n",
    "DATA_DIR = '../data'\n",
    "RESULTS_DIR = '../results'\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "# CONFIG\n",
    "WINDOW_SIZE = 5\n",
    "PMI_FREQ_THRESHOLD = 5\n",
    "\n",
    "# We will focus on ONE book for the presentation graphs to keep it clean\n",
    "TARGET_BOOK = \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\"\n",
    "MAIN_CHAR = \"Anna\"\n",
    "COMPARE_CHAR = \"Levin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    full_token_list = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        clean_words = []\n",
    "        for w in words:\n",
    "            if w.isalnum():\n",
    "                w_lower = w.lower()\n",
    "                if w_lower not in stop_words:\n",
    "                    lemma = lemmatizer.lemmatize(w_lower)\n",
    "                    clean_words.append(lemma)\n",
    "                    full_token_list.append(lemma)\n",
    "        if clean_words:\n",
    "            processed_sentences.append(clean_words)\n",
    "    return full_token_list, processed_sentences\n",
    "\n",
    "def get_collocates(target_name, sentences, window=5):\n",
    "    target = target_name.lower()\n",
    "    collocates = []\n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            if word == target:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sent), i + window + 1)\n",
    "                collocates.extend(sent[start:i] + sent[i+1:end])\n",
    "    return collocates\n",
    "\n",
    "def calculate_stats(collocates, full_tokens):\n",
    "    collocate_counts = Counter(collocates)\n",
    "    corpus_counts = Counter(full_tokens)\n",
    "    total_collocates = len(collocates)\n",
    "    total_corpus = len(full_tokens)\n",
    "    \n",
    "    stats = []\n",
    "    for word, count in collocate_counts.items():\n",
    "        if count < PMI_FREQ_THRESHOLD:\n",
    "            continue\n",
    "        p_w_given_c = count / total_collocates\n",
    "        p_w = corpus_counts[word] / total_corpus\n",
    "        if p_w > 0:\n",
    "            pmi = math.log(p_w_given_c / p_w)\n",
    "            stats.append({'word': word, 'freq': count, 'pmi': pmi})\n",
    "    \n",
    "    return pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Presentation Graphs\n",
    "This section creates 3 specific graphs: **Scatter Plot, Grouped Bar Chart, and Word Cloud**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_presentation_graphs(df_main, df_compare, main_name, compare_name):\n",
    "    \n",
    "    # --- GRAPH 1: SCATTER PLOT (Frequency vs. PMI) ---\n",
    "    # Topic: \"Common vs. Unique Vocabulary\"\n",
    "    print(\"Generating Graph 1: Scatter Plot...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Filter for readability (top 50 by frequency)\n",
    "    subset = df_main.sort_values(by='freq', ascending=False).head(40)\n",
    "    \n",
    "    plt.scatter(subset['freq'], subset['pmi'], color='purple', alpha=0.6, s=100)\n",
    "    \n",
    "    # Label points\n",
    "    for i, row in subset.iterrows():\n",
    "        plt.text(row['freq']+0.5, row['pmi'], row['word'], fontsize=9)\n",
    "        \n",
    "    plt.title(f\"Graph 1: Word Usage Distribution for {main_name}\\n(Frequency vs. Uniqueness)\", fontsize=14)\n",
    "    plt.xlabel(\"Frequency (Count)\")\n",
    "    plt.ylabel(\"PMI Score (Uniqueness)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/presentation_graph_1_scatter.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPH 2: GROUPED BAR CHART ---\n",
    "    # Topic: \"Thematic Contrast between Characters\"\n",
    "    print(\"Generating Graph 2: Comparison Bar Chart...\")\n",
    "    \n",
    "    # Select interesting thematic words to compare\n",
    "    comparison_words = ['love', 'home', 'life', 'wife', 'eye', 'hand', 'smile', 'guilt']\n",
    "    \n",
    "    # Extract counts\n",
    "    counts_main = [df_main[df_main['word'] == w]['freq'].sum() for w in comparison_words]\n",
    "    counts_compare = [df_compare[df_compare['word'] == w]['freq'].sum() for w in comparison_words]\n",
    "    \n",
    "    x = np.arange(len(comparison_words))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, counts_main, width, label=main_name, color='skyblue')\n",
    "    plt.bar(x + width/2, counts_compare, width, label=compare_name, color='salmon')\n",
    "    \n",
    "    plt.title(f\"Graph 2: Thematic Comparison ({main_name} vs. {compare_name})\", fontsize=14)\n",
    "    plt.xticks(x, comparison_words, fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(f\"{RESULTS_DIR}/presentation_graph_2_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- GRAPH 3: WORD CLOUD ---\n",
    "    # Topic: \"Visual Summary\"\n",
    "    print(\"Generating Graph 3: Word Cloud...\")\n",
    "    \n",
    "    # Convert dataframe to dict for wordcloud\n",
    "    freq_dict = dict(zip(df_main['word'], df_main['freq']))\n",
    "    \n",
    "    wc = WordCloud(width=800, height=400, background_color='white', colormap='magma').generate_from_frequencies(freq_dict)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Graph 3: Context Cloud for {main_name}\", fontsize=14)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/presentation_graph_3_wordcloud.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Analysis\n",
    "Execute the analysis and generate the 3 presentation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading and processing text...\")\n",
    "text_path = os.path.join(DATA_DIR, TARGET_BOOK)\n",
    "raw_text = load_text(text_path)\n",
    "\n",
    "if raw_text:\n",
    "    full_tokens, processed_sentences = clean_and_tokenize(raw_text)\n",
    "    \n",
    "    # Analyze Main Character (Anna)\n",
    "    print(f\"Analyzing {MAIN_CHAR}...\")\n",
    "    col_main = get_collocates(MAIN_CHAR, processed_sentences, WINDOW_SIZE)\n",
    "    df_main = calculate_stats(col_main, full_tokens)\n",
    "    \n",
    "    # Analyze Comparison Character (Levin)\n",
    "    print(f\"Analyzing {COMPARE_CHAR}...\")\n",
    "    col_compare = get_collocates(COMPARE_CHAR, processed_sentences, WINDOW_SIZE)\n",
    "    df_compare = calculate_stats(col_compare, full_tokens)\n",
    "    \n",
    "    # Generate Graphs\n",
    "    create_presentation_graphs(df_main, df_compare, MAIN_CHAR, COMPARE_CHAR)\n",
    "    \n",
    "    print(\"\\nDone! 3 presentation graphs saved to 'results' folder.\")\n",
    "else:\n",
    "    print(\"Could not load text. Check data path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}